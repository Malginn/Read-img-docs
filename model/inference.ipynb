{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHXRx7yXoAUe"
      },
      "source": [
        "-------------------------------------------\n",
        "# File with training and model inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZir1nnCbdKp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import shutil\n",
        "import cv2\n",
        "import PIL\n",
        "import pytesseract\n",
        "\n",
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsHDap2bbdKq"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---------------------------------------\n",
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 15\n",
        "IMGSZ = (1109, 1280)\n",
        "BATCH_SIZE = 32\n",
        "model_path = r\"C:\\Users\\ivano\\Desktop\\winwinhack\\model\\runs\\detect\\train6\\weights\\best.pt\"\n",
        "yaml_path = r\"C:\\Users\\ivano\\Desktop\\winwinhack\\gen_dataset\\YOLO_dataset\\data.yaml\"\n",
        "\n",
        "\n",
        "model = YOLO(model_path)\n",
        "model.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "id": "vOgYUE6lbdKr",
        "outputId": "1d51cf88-0dea-4040-a273-3fa1894285b1"
      },
      "outputs": [],
      "source": [
        "results = model.train(data=yaml_path, epochs=EPOCHS, imgsz=IMGSZ,  workers=0, show_labels=False, batch=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU4VkIqPkCnD"
      },
      "outputs": [],
      "source": [
        "metrics = model.val()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwj1snXrbdKv"
      },
      "source": [
        "### In this case we are interested in: `IoU`, `mAP50`, `loss`,\n",
        "\n",
        "Further, when classes are added, the `class-specific AP` will be important, i.e. AP metric for each class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------------------------------------\n",
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_example_path = '../data_gagarin/data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_aA6oJUXs-V",
        "outputId": "7eaf765c-cd59-4064-a0c6-4c442bf64939"
      },
      "outputs": [],
      "source": [
        "model = YOLO(r\"C:\\Users\\ivano\\Desktop\\winwinhack\\best_model\\best.pt\")\n",
        "pred = model.predict([test_example_path + i for i in os.listdir(test_example_path)[:10]], show_labels=True, save=True, imgsz=IMGSZ) #conf, iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def enhance_image(image):\n",
        "    r, g, b = cv2.split(image)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # 2\n",
        "\n",
        "    enhanced_r = clahe.apply(r)\n",
        "    enhanced_g = clahe.apply(g)\n",
        "    enhanced_b = clahe.apply(b)\n",
        "\n",
        "    enhanced_image = cv2.merge((enhanced_r, enhanced_g, enhanced_b))\n",
        "    enhanced_image = cv2.convertScaleAbs(enhanced_image, alpha=1.5, beta=0)\n",
        "\n",
        "    return enhanced_image\n",
        "\n",
        "\n",
        "# 3 канала\n",
        "def rework(image):\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_rgb = enhance_image(image_rgb)\n",
        "    pixels = image_rgb.reshape((-1, 3))\n",
        "    n_colors = 2\n",
        "    kmeans = KMeans(n_clusters=n_colors)\n",
        "    kmeans.fit(pixels)\n",
        "    main_colors = kmeans.cluster_centers_.astype(int)\n",
        "    most_common_color = main_colors[np.argmin(np.bincount(kmeans.labels_))]\n",
        "\n",
        "    treshold_post_class = 70\n",
        "\n",
        "    mask = np.any(np.abs(pixels - most_common_color) > treshold_post_class, axis=1)\n",
        "    result = np.where(mask.reshape(image.shape[:2]), 255, 0).astype(np.uint8)\n",
        "    result = cv2.bitwise_not(result)\n",
        "\n",
        "    _, result = cv2.threshold(result, 100, 255, cv2.THRESH_BINARY)  # 180\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cropp_data(pred):\n",
        "    global i\n",
        "    for iter in range(len(pred)):\n",
        "        try:\n",
        "            img = pred[iter].orig_img\n",
        "            x, y, x_1, y_1 = [round(i) for i in list(pred[iter].boxes.xyxy[0].to('cpu').numpy())]\n",
        "\n",
        "            roi_color = img[y:y_1, x:x_1]   #y:h, x:w\n",
        "            roi_color = rework(roi_color)\n",
        "\n",
        "\n",
        "            name = pred[iter].path.split('/')[-1]\n",
        "            cv2.imwrite(f'./results/{i}.jpg', roi_color)\n",
        "            i += 1\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "\n",
        "cropp_data(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(os.listdir('./results'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shutil.rmtree('./results')\n",
        "os.makedirs('./results')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# идет на python >=3.7 and <=3.11\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "from sklearn.cluster import KMeans\n",
        "from ultralytics import YOLO\n",
        "\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
        "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
        "\n",
        "\n",
        "IMGSZ = (1120, 1280)\n",
        "# путь до папки с классифицированными изображениями\n",
        "path_to_imgs = \"../data_gagarin/data/\"\n",
        "imgs_to_text = './results/'\n",
        "\n",
        "img_paths = [path_to_imgs + i for i in os.listdir(path_to_imgs)]\n",
        "img_to_text_paths = [imgs_to_text + i for i in os.listdir(imgs_to_text)]\n",
        "\n",
        "model_path = r\"C:\\Users\\ivano\\Desktop\\winwinhack\\best_model\\best.pt\"\n",
        "\n",
        "custom_config = r\"tessedit_char_whitelist=0123456789 --oem 3 --psm 7 --dpi 96\"   # \n",
        "# custom_config_alpha = r\"tessedit_char_whitelist=0123456789ABEKMNOPCTYX --dpi 100\"   # --user-words user_word.txt\n",
        "\n",
        "\n",
        "def enhance_image(image):\n",
        "    r, g, b = cv2.split(image)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # 2\n",
        "\n",
        "    enhanced_r = clahe.apply(r)\n",
        "    enhanced_g = clahe.apply(g)\n",
        "    enhanced_b = clahe.apply(b)\n",
        "\n",
        "    enhanced_image = cv2.merge((enhanced_r, enhanced_g, enhanced_b))\n",
        "    enhanced_image = cv2.convertScaleAbs(enhanced_image, alpha=1.5, beta=0)\n",
        "\n",
        "    return enhanced_image\n",
        "\n",
        "\n",
        "# 3 канала\n",
        "def rework(image):\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_rgb = enhance_image(image_rgb)\n",
        "    pixels = image_rgb.reshape((-1, 3))\n",
        "    n_colors = 2\n",
        "    kmeans = KMeans(n_clusters=n_colors)\n",
        "    kmeans.fit(pixels)\n",
        "    main_colors = kmeans.cluster_centers_.astype(int)\n",
        "    most_common_color = main_colors[np.argmin(np.bincount(kmeans.labels_))]\n",
        "\n",
        "    treshold_post_class = 70\n",
        "\n",
        "    mask = np.any(np.abs(pixels - most_common_color) > treshold_post_class, axis=1)\n",
        "    result = np.where(mask.reshape(image.shape[:2]), 255, 0).astype(np.uint8)\n",
        "    result = cv2.bitwise_not(result)\n",
        "\n",
        "    _, result = cv2.threshold(result, 100, 255, cv2.THRESH_BINARY)  # 180\n",
        "    return result\n",
        "\n",
        "\n",
        "def cropp_imgs_to_text(preds, config):  #\n",
        "    \"\"\"\n",
        "    Функция для вырезания предсказанных Bounding boxes\n",
        "    из исходных изображений\n",
        "\n",
        "    input -> предсказания\n",
        "    output -> папка results с обрезанными картинками\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    texts_dict = {}\n",
        "    for iter in range(len(preds)):\n",
        "        img = preds[iter].orig_img\n",
        "        try:\n",
        "            x, y, x_1, y_1 = [\n",
        "                round(i) for i in list(preds[iter].boxes.xyxy[0].to(\"cpu\").numpy())\n",
        "            ]\n",
        "\n",
        "            roi_color = img[y:y_1, x:x_1]\n",
        "            roi_color = rework(roi_color)\n",
        "            # print(roi_color)\n",
        "            name = preds[iter].path.split(\"/\")[-1]\n",
        "            # cv2.imwrite(f\"./results/{name[:-4]}.jpg\", roi_color)\n",
        "            text = pytesseract.image_to_string(roi_color, lang='eng', config=config)   #\n",
        "            texts.append(''.join(c if c.isdigit() else '' for c in text))\n",
        "            texts_dict[name] = ''.join(c if c.isdigit() else '' for c in text)      # or c.isalpha()\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return texts, texts_dict\n",
        "\n",
        "\n",
        "# инициализируем модель и загружаем веса\n",
        "model = YOLO(model_path)\n",
        "# делаем предсказания\n",
        "preds = model.predict(img_paths[:50], save=True, imgsz=IMGSZ)\n",
        "# вырезаем и сохраняем картинки\n",
        "texts, texts_dict = cropp_imgs_to_text(preds, custom_config)   #"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
